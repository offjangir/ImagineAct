# Repository Structure Guide

This document explains the organization of the Randomized-Return-Decomposition repository, including where neural networks, environment simulators, and training logic are located.

## ğŸ“ Directory Structure

```
Randomized-Return-Decomposition/
â”œâ”€â”€ algorithm/              # Neural network models and RL algorithms
â”‚   â”œâ”€â”€ basis_alg/         # Base RL algorithms (SAC, DDPG, TD3, DQN)
â”‚   â”‚   â”œâ”€â”€ base_torch.py  # Base class for PyTorch algorithms
â”‚   â”‚   â”œâ”€â”€ sac_torch.py   # SAC implementation (PyTorch)
â”‚   â”‚   â”œâ”€â”€ ddpg_torch.py  # DDPG implementation (PyTorch)
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ rrd_torch.py       # Randomized Reward Decomposition (PyTorch)
â”‚   â”œâ”€â”€ rrd.py             # RRD (TensorFlow - original)
â”‚   â””â”€â”€ replay_buffer/      # Experience replay buffers
â”œâ”€â”€ envs/                   # Environment simulators
â”‚   â”œâ”€â”€ normal_mujoco.py   # MuJoCo physics simulator wrapper
â”‚   â”œâ”€â”€ normal_atari.py    # Atari game environment wrapper
â”‚   â””â”€â”€ ep_rews.py         # Episodic rewards wrapper
â”œâ”€â”€ learner/                # Training loop logic
â”‚   â”œâ”€â”€ mujoco.py          # MuJoCo-specific training loop
â”‚   â””â”€â”€ atari.py           # Atari-specific training loop
â”œâ”€â”€ utils/                  # Utility functions
â”‚   â”œâ”€â”€ torch_utils.py     # PyTorch utilities (normalizers, etc.)
â”‚   â””â”€â”€ os_utils.py        # Logging and OS utilities
â”œâ”€â”€ scripts/                # Executable scripts
â”‚   â”œâ”€â”€ train.py           # Main training script
â”‚   â”œâ”€â”€ test.py            # Testing/evaluation script
â”‚   â””â”€â”€ *.sh               # Installation scripts
â”œâ”€â”€ requirements/           # Dependency files
â””â”€â”€ docs/                   # Documentation
```

## ğŸ§  Neural Network Models

### Location: `algorithm/` directory

The neural network architectures are defined in the algorithm files:

#### 1. **Base Algorithms** (`algorithm/basis_alg/`)

**PyTorch Implementations:**
- **`base_torch.py`**: Base class for all PyTorch algorithms
  - Handles device management (CPU/GPU)
  - Observation normalization
  - Model saving/loading

- **`sac_torch.py`**: Soft Actor-Critic (SAC) algorithm
  - **Policy Network** (`MLPStochasticPolicy`): 
    - 3-layer MLP: `obs_dim â†’ 256 â†’ 256 â†’ act_dim*2`
    - Outputs mean and logstd for stochastic policy
  - **Q-Value Networks** (`MLPQValueSAC`):
    - 3-layer MLP: `(obs_dim + act_dim) â†’ 256 â†’ 256 â†’ 1`
    - Two Q-networks for double Q-learning
    - Target networks for stability

- **`ddpg_torch.py`**: Deep Deterministic Policy Gradient
  - Similar architecture to SAC but with deterministic policy

#### 2. **RRD Algorithm** (`algorithm/rrd_torch.py`)

**Reward Decomposition Networks:**
- **`MLPRewardNet`**: For continuous control (MuJoCo)
  - Input: `[obs, action, obs - obs_next]`
  - Architecture: `state_dim â†’ 256 â†’ 256 â†’ 1`
  - Predicts decomposed rewards

- **`ConvRewardNet`**: For image-based tasks (Atari)
  - Convolutional layers: `32 â†’ 64 â†’ 64 filters`
  - Fully connected: `512 â†’ act_num`
  - Processes image observations

**Key Components:**
- `RRD` class wraps a basis algorithm (SAC/DDPG) and adds reward decomposition
- Reward network learns to predict rewards from state-action pairs
- Used for randomized reward decomposition learning

### Network Architecture Summary

```
SAC Policy Network:
  obs (obs_dim) 
    â†’ Linear(256) + ReLU
    â†’ Linear(256) + ReLU  
    â†’ Linear(act_dim*2)  [mean, logstd]

SAC Q-Network:
  [obs, action] (obs_dim + act_dim)
    â†’ Linear(256) + ReLU
    â†’ Linear(256) + ReLU
    â†’ Linear(1)  [Q-value]

RRD Reward Network (MLP):
  [obs, action, obs_diff] (obs_dim*2 + act_dim)
    â†’ Linear(256) + ReLU
    â†’ Linear(256) + ReLU
    â†’ Linear(1)  [reward]
```

## ğŸŒ Environment Simulators

### Location: `envs/` directory

#### 1. **MuJoCo Environments** (`envs/normal_mujoco.py`)

**`MuJoCoNormalEnv`** class:
- Wraps OpenAI Gym MuJoCo environments
- Supported environments:
  - `Ant-v2`, `HalfCheetah-v2`, `Walker2d-v2`
  - `Humanoid-v2`, `Reacher-v2`, `Swimmer-v2`
  - `Hopper-v2`, `HumanoidStandup-v2`

**Key Methods:**
- `reset()`: Reset environment to initial state
- `step(action)`: Execute action, return (obs, reward, done, info)
- `get_obs()`: Get current observation
- Handles both old and new Gym API formats

**Physics Engine:**
- Uses `gym.make(env_name)` which loads MuJoCo physics
- MuJoCo runs on **CPU** (not GPU)
- This is why GPU utilization is low during training

#### 2. **Atari Environments** (`envs/normal_atari.py`)

**`AtariNormalEnv`** class:
- Wraps OpenAI Gym Atari environments
- Handles frame stacking and preprocessing
- Supports all standard Atari games

#### 3. **Environment Factory** (`envs/__init__.py`)

**`make_env(args)`** function:
- Creates appropriate environment based on `args.env`
- Returns wrapped environment with episodic rewards if needed
- Maps environment names to categories (atari/mujoco)

## ğŸ”„ Training Loop

### Location: `learner/` directory

#### **MuJoCo Learner** (`learner/mujoco.py`)

**`MuJoCoLearner`** class:
- Manages the training loop for MuJoCo environments
- **Key Process:**
  1. **Data Collection**: 
     - Agent interacts with environment
     - Stores transitions in replay buffer
  2. **Training**:
     - Samples batches from replay buffer
     - Updates policy and Q-networks
     - Updates target networks (for stability)
  3. **Logging**:
     - Tracks episodes, timesteps, rewards

**Training Flow:**
```
For each iteration:
  For each timestep:
    action = agent.step(obs, explore=True)
    obs, reward, done, info = env.step(action)
    buffer.store_transition(obs, action, reward, ...)
    
    if buffer.size >= warmup:
      for train_batches:
        batch = buffer.sample_batch()
        agent.train(batch)  # Update networks
```

## ğŸš€ Entry Points

### Main Scripts: `scripts/` directory

1. **`train.py`**: Main training script
   - Parses arguments via `common.py`
   - Sets up environment, agent, buffer, learner
   - Runs training loop with logging

2. **`test.py`**: Evaluation script
   - Loads trained model
   - Runs evaluation rollouts
   - Reports performance metrics

3. **`common.py`**: Configuration and setup
   - Argument parsing
   - Creates environment, agent, buffer, learner
   - Initializes logging

## ğŸ“Š Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Environmentâ”‚ (MuJoCo/Atari simulator)
â”‚  (envs/)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ obs, reward, done
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Agent     â”‚ (Neural networks)
â”‚ (algorithm/)â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ action
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Buffer    â”‚ (Experience replay)
â”‚(replay_buf/)â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ batch samples
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Learner   â”‚ (Training loop)
â”‚  (learner/) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”‘ Key Files Summary

| Component | Location | Purpose |
|-----------|----------|---------|
| **Neural Networks** | `algorithm/basis_alg/*_torch.py` | Policy and Q-value networks |
| **Reward Networks** | `algorithm/rrd_torch.py` | Reward decomposition networks |
| **Environment** | `envs/normal_mujoco.py` | MuJoCo physics simulator wrapper |
| **Training Loop** | `learner/mujoco.py` | Data collection and training logic |
| **Replay Buffer** | `algorithm/replay_buffer/` | Experience storage and sampling |
| **Main Script** | `scripts/train.py` | Entry point for training |
| **Config** | `common.py` | Argument parsing and setup |

## ğŸ’¡ Important Notes

1. **GPU vs CPU**: 
   - Neural networks run on GPU (if available)
   - MuJoCo physics runs on CPU (this is why GPU utilization is low)

2. **Backend Support**:
   - PyTorch implementations in `*_torch.py` files
   - Original TensorFlow implementations in `*.py` files (no `_torch` suffix)
   - Controlled by `USE_PYTORCH` environment variable

3. **Architecture**:
   - All networks are **MLPs** (Multi-Layer Perceptrons)
   - No convolutional layers for MuJoCo (vector observations)
   - Convolutional layers only for Atari (image observations)

4. **Training Speed**:
   - Bottleneck is CPU-based MuJoCo simulation
   - Not GPU computation (networks are small MLPs)
   - Parallelization helps but is limited by CPU cores

