import numpy as np
from PIL import Image
import torch
import base64
import re
import cv2
# from openai import OpenAI
# from google import genai
import einops
from pytorchvideo.data.encoded_video import EncodedVideo
from torchvision import transforms

import os
from pathlib import Path
import json

def rescale_bridge_action(
    a,
    wv_lo=-0.05,
    wv_hi=+0.05,
    wv_post_scale_max=+1.75,
    wv_post_scale_min=-1.75,
    rd_lo=-0.25,
    rd_hi=+0.25,
    rd_post_scale_max=+1.4,
    rd_post_scale_min=-1.4):
    """
    Rescale Bridge (WidowX) action to the ranges expected by the world model.
    We need to call this function on the unnormalized action values returned by the policy.
    """
    # rescale end effector
    a[:3] = (a[:3] - wv_lo) / (wv_hi - wv_lo) * (
        wv_post_scale_max - wv_post_scale_min
    ) + wv_post_scale_min
    a[:3] = torch.clamp(a[:3], wv_post_scale_min, wv_post_scale_max)
    # rescale joint rotations
    a[3:6] = (a[3:6] - rd_lo) / (rd_hi - rd_lo) * (
        rd_post_scale_max - rd_post_scale_min
    ) + rd_post_scale_min
    a[3:6] = torch.clamp(a[3:6], rd_post_scale_min, rd_post_scale_max)
    # threshold the gripper
    a[6] = torch.where(a[6] > 0.8, -1.0, +1.0)
    return a

def encode_video(video, stride=20):
    frames, idx = [], 0
    for idx, frame in enumerate(video):
        if idx % stride == 0:
            if (frame == 0).all():
                break
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            _, buf = cv2.imencode(".jpg", frame)
            frames.append(base64.b64encode(buf).decode())
    return frames

def evaluate(scores):
    partial = scores[:, 0]  # First subtask scores
    complete = scores[:, 1]  # Second subtask scores
    print(f"Partial completion mean score: {np.round(100*np.mean(partial))=}")
    print(f"Partial completion STE: {np.round(100*np.std(partial) / len(partial)**0.5)=}")
    print(f"Completion mean score: {np.round(100*np.mean(complete))=}")
    print(f"Completion STE: {np.round(100*np.std(complete) / len(complete)**0.5)=}")


# def predict(video, trial, n=5):
#     frames = encode_video(video)
#     instruction = trial["instruction"]
#     has_partial = bool(trial.get("partial_criteria"))
#     if has_partial:
#         partial_desc = trial.get("partial_criteria")
#         rubric = f"""
# Score rubric (choose exactly ONE number):
# 0   = Failure: little or no progress toward: "{instruction}"
# 0.5 = Partial: "{partial_desc}" achieved BUT the instruction not fully completed
# 1   = Success: Instruction fully completed (counts even if partial also true)
# """
#         output_spec = "Final Score: <0 | 0.5 | 1>"
#         allowed_pattern = r"(0\.5|0(?:\.0)?|1(?:\.0)?)"
#     else:
#         rubric = f"""
# Score rubric:
# 0 = Failure: instruction "{instruction}" not completed.
# 1 = Success: instruction completed."""
#         output_spec = "Final Score: <0 | 1>"
#         allowed_pattern = r"(0(?:\.0)?|1(?:\.0)?)"

#     prompt = f"""
# Here is a sequence of frames from a robot policy which has been rolled out in a video-generation-based world model.
# I need your help determining whether the policy is successful. How successfully does the robot complete the following task?

# Instruction: {instruction}
# {rubric.strip()}

# Provide brief reasoning (2-3 sentences). Then output EXACTLY one final line:
# Final Score: X
# Where X is { 'one of 0, 0.5, or 1' if has_partial else '0 or 1' }.
# No extra numbers after that line.
# Note: Since this video was generated by a video prediction model (conditioned on robot actions), it may contain some artifacts due to the video model capacity.
# """.strip()

#     client = genai.Client(api_key="")
    
#     # Prepare content for Gemini API - combine text prompt with images
#     content_parts = [prompt]
#     for frame_b64 in frames:
#         # Use the Part type from genai to properly structure image data
#         from google.genai import types
#         content_parts.append(
#             types.Part.from_bytes(
#                 data=base64.b64decode(frame_b64),
#                 mime_type="image/jpeg"
#             )
#         )
    
#     counts = {"0": 0, "0.5": 0, "1": 0}
#     parsed = 0
#     pattern = re.compile(rf"Final\s*Score:\s*{allowed_pattern}", re.IGNORECASE)
    
#     # Generate n responses
#     for i in range(n):
#         try:
#             response = client.models.generate_content(
#                 model="gemini-2.0-flash-exp",
#                 contents=content_parts
#             )
#             content = response.text.strip()
            
#             m = pattern.search(content)
#             if not m:
#                 print(f"No match for attempt {i+1}:", repr(content[:200]))
#                 continue
#             val = m.group(1).strip()
#             if val in ("0.0", "0"): key = "0"
#             elif val in ("1.0", "1"): key = "1"
#             elif val in ("0.5",): key = "0.5"
#             else:
#                 print("Unrecognized value:", val)
#                 continue
#             counts[key] += 1
#             parsed += 1
#         except Exception as e:
#             print(f"Error in attempt {i+1}: {e}")
#             continue

#     if parsed == 0:
#         print("No valid parses; returning 0.")
#         return 0.0

#     # Majority selection; tie-break preference: 1 > 0.5 > 0
#     ordered = ["1", "0.5", "0"] if has_partial else ["1", "0"]
#     best_key = max(ordered, key=lambda k: (counts[k], ordered.index(k)*-1))
#     score = {"0": 0.0, "0.5": 0.5, "1": 1.0}[best_key]
#     print(f"Parsed {parsed}/{n}. Vote counts {counts} -> score {score}")
#     return score

def load_tasks(root):
    for file in os.listdir(root):
        if file.endswith(".png"):
            base = os.path.splitext(file)[0]
            yield os.path.join(root, base)

def _titleize(name: str):
    name = name.replace("--", " ")
    return " ".join(w.capitalize() for w in re.split(r"[_\\-]+", name))

def discover_trials_from_videos(root_dir):
    """Discover trials from video files and extract first frame using pytorchvideo."""
    root_path = Path(root_dir).resolve()
    trials = []
    
    for video_path in list(root_path.rglob("*.mp4")) + list(root_path.rglob("*.avi")):
        task_dir = video_path.parent
        base = video_path.stem
        json_path = task_dir / f"{base}.json"
        
        if not json_path.exists():
            print(f"[WARN] No JSON for {video_path}")
            continue
        
        try:
            meta = json.loads(json_path.read_text())
        except Exception as e:
            print(f"[WARN] Bad JSON {json_path}: {e}")
            continue
        
        instruction = meta.get("instruction")
        if not instruction:
            print(f"[WARN] No instruction in {json_path}")
            continue
        
        # Load video and extract first frame using pytorchvideo
        try:
            encoded_video = EncodedVideo.from_path(str(video_path))
            # Get the first frame (start at 0, get minimal duration)
            video_data = encoded_video.get_clip(start_sec=0.0, end_sec=0.1)
            # video_data['video'] shape: (C, T, H, W)
            first_frame_tensor = video_data['video'][:, 0, :, :]  # (C, H, W)
            # Convert from (C, H, W) to (H, W, C) - keep as float32 in [0, 1] range
            transform = transforms.Resize((int(256), int(256)))
            first_frame_tensor = transform(first_frame_tensor)
            first_frame_tensor = einops.rearrange(first_frame_tensor, 'c h w -> h w c')
            first_frame = first_frame_tensor.cpu().numpy()
            
        except Exception as e:
            print(f"[WARN] Failed to extract first frame from {video_path}: {e}")
            continue
        
        partial = meta.get("partial_credit_criteria")
        task_key = str(task_dir.relative_to(root_path))
        trials.append({
            "trial_video": str(video_path),
            "first_frame": first_frame,
            "instruction": instruction,
            "partial_criteria": partial,
            "task_key": task_key,
            "task_display": _titleize(task_dir.name),
        })
    
    return trials

def discover_trials(root_dir):
    root_path = Path(root_dir).resolve()
    trials = []
    for png in list(root_path.rglob("*.png")) + list(root_path.rglob("*.jpg")):
        task_dir = png.parent
        base = png.stem
        json_same = task_dir / f"{base}.json"
        meta_path = json_same if json_same.exists() else None
        if not meta_path:
            print(f"[WARN] No JSON for {png}")
            continue
        try:
            meta = json.loads(meta_path.read_text())
        except Exception as e:
            print(f"[WARN] Bad JSON {meta_path}: {e}")
            continue
        instruction = meta.get("instruction")
        if not instruction:
            print(f"[WARN] No instruction in {meta_path}")
            continue
        partial = meta.get("partial_credit_criteria")
        task_key = str(task_dir.relative_to(root_path))
        trials.append({
            "trial_png": str(png),
            "instruction": instruction,
            "partial_criteria": partial,
            "task_key": task_key,
            "task_display": _titleize(task_dir.name),
        })
    return trials

def aggregate_model_results(results):
    tasks = {}
    for r in results:
        key = r["task_key"]
        if key not in tasks:
            tasks[key] = {
                "Task": r["task_display"],
                "# Trials": 0,
                "# Successes": 0.0,
            }
        tasks[key]["# Trials"] += 1
        tasks[key]["# Successes"] += float(r["score"])

    task_list = sorted(tasks.values(), key=lambda x: x["Task"])

    per_trial_scores = []
    for t in task_list:
        trials = t["# Trials"]
        succ = t["# Successes"]
        if trials > 0:
            per_trial_scores.extend([succ / trials] * trials)
    per_trial_scores = np.array(per_trial_scores)
    N = len(per_trial_scores)
    mean_rate = 100.0 * per_trial_scores.mean() if N else 0.0
    ste = 100.0 * (per_trial_scores.std() / np.sqrt(N)) if N else 0.0
    return {"tasks": task_list, "mean_success_rate": mean_rate, "ste": ste}

def print_results_table(agg):
    tasks = agg["tasks"]
    header = ["Task", "# Trials", "# Successes"]
    col_w = [len(h) for h in header]
    for t in tasks:
        col_w[0] = max(col_w[0], len(t["Task"]))
        col_w[1] = max(col_w[1], len(str(t["# Trials"])))
        col_w[2] = max(col_w[2], len(str(t["# Successes"])))
    fmt = lambda row: " | ".join(str(v).ljust(col_w[i]) for i, v in enumerate(row))
    print(fmt(header))
    print("-" * (sum(col_w) + 3 * (len(header) - 1)))
    for t in tasks:
        print(fmt([
            t["Task"],
            t["# Trials"],
            t["# Successes"]
        ]))
    print("-" * (sum(col_w) + 3 * (len(header) - 1)))
    print(f"Mean Success Rate: {agg['mean_success_rate']:.1f}Â±{agg['ste']:.1f}%")